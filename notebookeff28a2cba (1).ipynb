{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11199196,"sourceType":"datasetVersion","datasetId":6992262}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è GPU\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU is not available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:51:13.648977Z","iopub.execute_input":"2025-03-29T15:51:13.649308Z","iopub.status.idle":"2025-03-29T15:51:13.655497Z","shell.execute_reply.started":"2025-03-29T15:51:13.649280Z","shell.execute_reply":"2025-03-29T15:51:13.654788Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\nNumber of GPUs: 2\nCurrent GPU: Tesla T4\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!pip install transformers diffusers accelerate peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:46:06.153102Z","iopub.execute_input":"2025-03-29T16:46:06.153364Z","iopub.status.idle":"2025-03-29T16:46:10.706720Z","shell.execute_reply.started":"2025-03-29T16:46:06.153344Z","shell.execute_reply":"2025-03-29T16:46:10.705824Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (11.0.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install langdetect\nimport os\nimport torch\nfrom diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom langdetect import detect\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom accelerate import Accelerator\nfrom peft import LoraConfig, get_peft_model\nfrom tqdm.auto import tqdm\n\n# –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU\ntorch.cuda.empty_cache()\n\n# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:46:10.708184Z","iopub.execute_input":"2025-03-29T16:46:10.708551Z","iopub.status.idle":"2025-03-29T16:46:37.308947Z","shell.execute_reply.started":"2025-03-29T16:46:10.708514Z","shell.execute_reply":"2025-03-29T16:46:37.307980Z"}},"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=8fef49aa612ebfbb48efe23944ec884d4a2d6858935788345abb059dec466e5f\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n","output_type":"stream"},{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f62f0a9bc6b4ecbb74ae87a8151d67d"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Stable Diffusion 2.1\npipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16)\npipe.to(device)\n\n# –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π —Å—ç–º–ø–ª–µ—Ä –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:46:43.104704Z","iopub.execute_input":"2025-03-29T16:46:43.105131Z","iopub.status.idle":"2025-03-29T16:47:03.149553Z","shell.execute_reply.started":"2025-03-29T16:46:43.105094Z","shell.execute_reply":"2025-03-29T16:47:03.148521Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/543 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ec9d4dc1d34782bd0e243c57b44ade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f096e32a6940e89137989bbd437a00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230ceae837f9481abcc1f72b402466cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebef4e4978641b58822b41a8a9afffb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e89778f11524e3db938a03ac40ce802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5551ffd3a934182a9e06c1822d8e942"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/807 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f13e428e544f5b853103b068acbac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eacde76c64054970b6aff3ebc661e87e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/911 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d2ff9e4a1674449a1a5d6f197f28dfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d7151c213d045bfad34d686962415af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea4115aaf6843618abee60a3b2d885f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a794a3b919b24ec9a50defcf6af5ee42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b69b20e442a4485381a8724432a3c4c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d789d7b84355440f9f13de847a50f839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d61d9999214ba2bae090c251fba94c"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def translate_ru_to_en(text):\n    model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ GPU\n\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    translated = model.generate(**inputs)\n    return tokenizer.decode(translated[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:46:37.317126Z","iopub.execute_input":"2025-03-29T16:46:37.317474Z","iopub.status.idle":"2025-03-29T16:46:37.331105Z","shell.execute_reply.started":"2025-03-29T16:46:37.317443Z","shell.execute_reply":"2025-03-29T16:46:37.330140Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('jpg', 'png', 'jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç RGB\n        if self.transform:\n            image = self.transform(image)\n        return {'image': image}\n\n# –§—É–Ω–∫—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è\ndef prepare_images(image_dir):\n    transform = transforms.Compose([\n        transforms.Resize((512, 512)),  # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ 512x512\n        transforms.ToTensor()          # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä\n    ])\n    dataset = ImageDataset(image_dir, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:49:20.746070Z","iopub.execute_input":"2025-03-29T16:49:20.746385Z","iopub.status.idle":"2025-03-29T16:49:20.752908Z","shell.execute_reply.started":"2025-03-29T16:49:20.746361Z","shell.execute_reply":"2025-03-29T16:49:20.751832Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from langdetect import detect\n\ndef process_prompt(user_input):\n    try:\n        lang = detect(user_input)\n    except:\n        lang = \"unknown\"\n\n    if lang == \"ru\":\n        translated_text = translate_ru_to_en(user_input)\n        print(f\"üîÑ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {translated_text}\")\n        return translated_text\n    else:\n        print(f\"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {user_input}\")\n        return user_input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:49:17.033664Z","iopub.execute_input":"2025-03-29T16:49:17.034090Z","iopub.status.idle":"2025-03-29T16:49:17.038908Z","shell.execute_reply.started":"2025-03-29T16:49:17.034060Z","shell.execute_reply":"2025-03-29T16:49:17.038004Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def fine_tune_lora(pipe, train_dataloader, prompt, num_epochs=3, lr=1e-4, output_dir=\"lora_weights\"):\n    print(\"üîß –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å LoRA...\")\n\n    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n        lora_dropout=0.05,\n        bias=\"none\"\n    )\n\n    pipe.unet = get_peft_model(pipe.unet, lora_config)\n    pipe.unet.print_trainable_parameters()\n\n    optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=lr)\n    scaler = GradScaler()\n    accelerator = Accelerator()\n    pipe.unet, optimizer, train_dataloader = accelerator.prepare(pipe.unet, optimizer, train_dataloader)\n\n    for epoch in range(num_epochs):\n        pipe.unet.train()\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n        for batch in progress_bar:\n            optimizer.zero_grad()\n\n            noise = torch.randn_like(batch['image']).to(device)\n            timesteps = torch.randint(0, 1000, (batch['image'].shape[0],), device=device).long()\n\n            text_input = [prompt] * batch['image'].shape[0]\n            text_input = pipe.tokenizer(text_input, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(device)\n            encoder_hidden_states = pipe.text_encoder(text_input).last_hidden_state\n\n            noisy_images = pipe.scheduler.add_noise(batch['image'], noise, timesteps)\n\n            with autocast():\n                noise_pred = pipe.unet(noisy_images, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n\n            loss = torch.nn.functional.mse_loss(noise_pred, noise)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            progress_bar.set_postfix(loss=loss.item())\n\n    os.makedirs(output_dir, exist_ok=True)\n    pipe.unet.save_pretrained(output_dir)\n    print(f\"‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{output_dir}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:49:09.938094Z","iopub.execute_input":"2025-03-29T16:49:09.938377Z","iopub.status.idle":"2025-03-29T16:49:09.946328Z","shell.execute_reply.started":"2025-03-29T16:49:09.938355Z","shell.execute_reply":"2025-03-29T16:49:09.945590Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generate_image(prompt, negative_prompt=\"low quality, blurry, poorly drawn, distorted, deformed, bad anatomy, bad proportions, watermark, text, nsfw\"):\n    image = pipe(prompt, negative_prompt=negative_prompt, guidance_scale=7.5).images[0]\n    image.save(\"generated_image.png\")\n    print(\"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ 'generated_image.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:29:56.902032Z","iopub.execute_input":"2025-03-29T16:29:56.902384Z","iopub.status.idle":"2025-03-29T16:29:56.906591Z","shell.execute_reply.started":"2025-03-29T16:29:56.902358Z","shell.execute_reply":"2025-03-29T16:29:56.905741Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n    image_dir = \"/kaggle/input/in-img\"  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\n    train_dataloader = prepare_images(image_dir)\n\n    # –ü—Ä–æ–º—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è\n    prompt_for_finetuning = \"A photo of a girl named Anya\"\n\n    # –î–æ–æ–±—É—á–µ–Ω–∏–µ LoRA\n    lora_weights_dir = \"lora_weights\"\n    fine_tune_lora(pipe, train_dataloader, prompt_for_finetuning, num_epochs=3, lr=1e-4, output_dir=lora_weights_dir)\n\n    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n    user_prompt = \"–§–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –¥–µ–≤—É—à–∫–∏ –ø–æ –∏–º–µ–Ω–∏ –ê–Ω—è, –Ω–∞ –ø–ª—è–∂–µ —Å –∫—Ä–∞–±–∞–º–∏, —Ö–æ—Ä–æ—à–µ–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\"\n    final_prompt = process_prompt(user_prompt)  # –ü–µ—Ä–µ–≤–æ–¥, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n    negative_prompt_default = \"low quality, blurry, poorly drawn, distorted, deformed, bad anatomy, bad proportions, watermark, text, nsfw\"\n    generate_image(final_prompt, negative_prompt_default)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:49:25.460687Z","iopub.execute_input":"2025-03-29T16:49:25.460998Z","iopub.status.idle":"2025-03-29T16:49:25.733420Z","shell.execute_reply.started":"2025-03-29T16:49:25.460972Z","shell.execute_reply":"2025-03-29T16:49:25.732221Z"}},"outputs":[{"name":"stdout","text":"üîß –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å LoRA...\ntrainable params: 3,319,808 || all params: 869,230,532 || trainable%: 0.3819\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-bdfc6b6d4675>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# –î–æ–æ–±—É—á–µ–Ω–∏–µ LoRA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlora_weights_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lora_weights\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfine_tune_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_for_finetuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_weights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-3bfd27257002>\u001b[0m in \u001b[0;36mfine_tune_lora\u001b[0;34m(pipe, train_dataloader, prompt, num_epochs, lr, output_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GradScaler' is not defined"],"ename":"NameError","evalue":"name 'GradScaler' is not defined","output_type":"error"}],"execution_count":16}]}